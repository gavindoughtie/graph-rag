{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph RAG\n",
    "\n",
    "Example of how to build and query a graph database for RAG.\n",
    "\n",
    "# Setup\n",
    "\n",
    "# install poetry if you don't have it yet\n",
    "`brew install poetry`\n",
    "\n",
    "# install docker\n",
    "If you're on a Mac, you'll want the docker desktop application, available here: https://www.docker.com/products/docker-desktop/\n",
    "\n",
    "# install langchain, etc. \n",
    "Make sure you're in the directory that holds `poetry.lock`\n",
    "\n",
    "`poetry install`\n",
    "\n",
    "`source $(poetry env info --path)/bin/activate`\n",
    "\n",
    "Copy `.env.example` to `.env` and update the variables as documented in that file.\n",
    "\n",
    "# run neo4j\n",
    "```\n",
    "docker run \\\n",
    "    -p 7474:7474 -p 7687:7687 \\\n",
    "    -v $PWD/data:/data -v $PWD/plugins:/plugins \\\n",
    "    --name neo4j-apoc \\\n",
    "    -e NEO4J_AUTH=$NEO4J_USERNAME/$NEO4J_PASSWORD \\\n",
    "    -e NEO4J_apoc_export_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n",
    "    -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\\n",
    "    -e NEO4J_dbms_security_procedures_unrestricted=apoc.\\\\\\* \\\n",
    "    neo4j:latest\n",
    "```\n",
    "(this command is in `run_neo4j_docker.sh`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Tuple, List, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  pass\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate neo4j\n",
    "OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_KEY = os.getenv('NEO4J_KEY')\n",
    "NEO4J_AUTH = (NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "\n",
    "with GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH) as driver:\n",
    "    driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the wikipedia articles\n",
    "article_keys = [\n",
    "  \"Star Trek: The Next Generation\", \n",
    "  \"List of Star Trek: The Next Generation episodes\",\n",
    "]\n",
    "extended_keys = [\n",
    "  \"Data\",\n",
    "  \"Wesley Crusher\",\n",
    "  \"Ro Laren\",\n",
    "  \"William Riker\",\n",
    "  \"Geordi La Forge\",\n",
    "  \"Deanna Troi\",\n",
    "  \"Guinan (Star Trek)\",\n",
    "  \"Beverly Crusher\",\n",
    "  \"Worf\",\n",
    "  \"Tasha Yar\",\n",
    "  \"Spock\",\n",
    "  \"Jean-Luc Picard\",\n",
    "  \"Miles O'Brien\",\n",
    "  \"Reginald Barclay\",\n",
    "  \"Deep Space Nine\"\n",
    "]\n",
    "\n",
    "# if you're a real Star Trek nerd, uncomment this line:\n",
    "#\n",
    "# article_keys.extend(extended_keys)\n",
    "#\n",
    "# be aware that this will add an hour or more of additional\n",
    "# node generation time in the \"convert_to_graph_documents\"\n",
    "# step, and it will cost some actual money if you're paying\n",
    "# for LLM access.\n",
    "\n",
    "raw_documents = []\n",
    "batch_size = 2\n",
    "current_key_index = 0\n",
    "while current_key_index < len(article_keys):\n",
    "  for i in range(current_key_index, batch_size)\n",
    "    query = article_keys[i]\n",
    "    raw_documents.extend(WikipediaLoader(query=query).load())\n",
    "    current_key_index += 1\n",
    "\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly show the graph resulting from the given Cypher query\n",
    "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    #display(widget)\n",
    "    return widget\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model_name=OPENAI_MODEL_NAME,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=OPENAI_BASE_URL,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the LLM to Build the Knowledge Graph\n",
    "## Then combine knowledge and vector search to create LLM prompt\n",
    "\n",
    "![Hybrid retrieval. Image from LangChain.](./68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f746f6d61736f6e6a6f2f626c6f67732f6d61737465722f67726170686879627269642e706e67.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "from langchain_community.graphs.neo4j_graph import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "\n",
    "def add_query(query):\n",
    "  text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "  raw_documents = WikipediaLoader(query=query).load()\n",
    "  print(f'{len(raw_documents)} raw documents for \"{query}\"')\n",
    "  documents = text_splitter.split_documents(raw_documents)\n",
    "  print(f'calling transformer to convert {len(documents)} documents')\n",
    "  graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "  print(f'Adding {len(graph_documents)} graph documents')\n",
    "  graph.add_graph_documents(\n",
    "      graph_documents,\n",
    "      baseEntityLabel=True,\n",
    "      include_source=True\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(queries):\n",
    "  from time import sleep\n",
    "  sleep_time = 1\n",
    "  attempting = True\n",
    "  for query in extended_keys:\n",
    "    attempting = True\n",
    "    sleep_time = 1\n",
    "    while attempting and sleep_time <= 32:\n",
    "      try:\n",
    "        add_query(query)\n",
    "        attempting = False\n",
    "      except:\n",
    "        print(f'error, sleeping for {sleep_time} seconds')\n",
    "        sleep(sleep_time)\n",
    "        sleep_time *= 2\n",
    "\n",
    "extended_keys = [\n",
    "  \"Data\",\n",
    "  \"Wesley Crusher\",\n",
    "  \"Ro Laren\",\n",
    "  \"William Riker\",\n",
    "  \"Geordi La Forge\",\n",
    "  \"Deanna Troi\",\n",
    "  \"Guinan (Star Trek)\",\n",
    "  \"Beverly Crusher\",\n",
    "  \"Worf\",\n",
    "  \"Tasha Yar\",\n",
    "  \"Spock\",\n",
    "  \"Jean-Luc Picard\",\n",
    "  \"Miles O'Brien\",\n",
    "  \"Reginald Barclay\",\n",
    "  \"Deep Space Nine\"\n",
    "]\n",
    "\n",
    "# build_graph(extended_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured data retriever\n",
    "You can use the `Neo4jVector.from_existing_graph` method to add both keyword and vector retrieval to documents. This method configures keyword and vector search indexes for a hybrid search approach, targeting nodes labeled `Document`. Additionally, it calculates text embedding values if they are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this USED to work but now does not!\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(), # this is probably incorrect if you're not using branded OpenAI!\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,    \n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph retriever\n",
    "On the other hand, configuring a graph retrieval is more involved but offers more freedom. In this example, we will use a full-text index to identify relevant nodes and then return their direct neighborhood.\n",
    "\n",
    "![Graph retriever. Image from LangChain.](./1_z0pYA_dSNG_yTYE6Rr7CQA.png)\n",
    "\n",
    "The graph retriever starts by identifying relevant entities in the input. For simplicity, we instruct the LLM to identify people, organizations, and locations. To achieve this, we will use LCEL with the newly added `with_structured_output` method to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities from text\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, character, organization, or starships \\\n",
    "        that appear in the text\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting character and person entities from the \\\n",
    "            text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the \\\n",
    "             following\"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets an entity\n",
    "entity_chain.invoke({\"question\": \"Who is Captain Jean-Luc Picard?\"}).names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we can detect entities in the question, let’s use a full-text index to map them to the knowledge graph. First, we need to define a full-text index and a function that will generate full-text queries that allow a bit of misspelling, which we won’t go into much detail here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text\n",
    "    search. It processes the input string by splitting it into words and \n",
    "    appending a similarity threshold (~2 changed characters) to each\n",
    "    word, then combines them using the AND operator. Useful for mapping\n",
    "    entities from user questions to database values, and allows for some \n",
    "    misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, \n",
    "            {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS \n",
    "              output\n",
    "              UNION\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS \n",
    "              output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `structured_retriever` function starts by detecting entities in the user question. Next, it iterates over the detected entities and uses a Cypher template to retrieve the neighborhood of relevant nodes. Let’s test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = structured_retriever(\"Who is Picard?\")\n",
    "print('\\n'.join(answers.split('\\n')[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the unstructured and graph retriever to create the final context that will be passed to an LLM.\n",
    "\n",
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows for follow-up questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the RAG chain\n",
    "We have successfully implemented the retrieval component of the RAG. Next, we introduce a prompt that leverages the context provided by the integrated hybrid retriever to produce the response, completing the implementation of the RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"Who was Picard's security officer?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow-up questions:\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What race is he?\",\n",
    "        \"chat_history\": [(\"Who was Picard's security officer?\", \"Picard's security officer was Worf, portrayed by Michael Dorn.\")],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
